# PG-Flow Implementation Plan
_Norm-based Parameter-Gated Flow Surrogate_

ì´ ë¬¸ì„œëŠ” **PG-Flow** ë°©ë²•ë¡ ì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•œ  
**ì‹¤ì§ˆì ì¸ ì•¡ì…˜ í”Œëœ**ì„ ì •ë¦¬í•œ ê²ƒì´ë‹¤.

ëª©í‘œ:

> í›ˆë ¨ëœ ëª¨ë¸ $(M)$ (ì˜ˆ: YOLO/DETR/Faster R-CNN ë“±)ì— ëŒ€í•´  
> **ì•„í‚¤í…ì²˜ DAG + weight ê¸°ë°˜ gate**ë¥¼ ì‚¬ìš©í•˜ëŠ”  
> **parameter-gated flow surrogate** $(s(M))$ ê³„ì‚° ëª¨ë“ˆì„ êµ¬í˜„í•œë‹¤.

PG-Flowì˜ ì „ì œ:

- weightë¥¼ ì´ìš©í•˜ì—¬ gate $(g_i)$ë¥¼ êµ¬ì„±í•œë‹¤.
- gateëŠ” **GÂ¹/GÂ²/GÂ³ íƒ€ì… ì¤‘ í•˜ë‚˜**, **gating íŒ¨í„´ì€ outgoing / incoming ì¤‘ í•˜ë‚˜**ë¥¼ ì„ íƒí•œë‹¤.

---

## 0. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

êµ¬í˜„í•˜ê³ ì í•˜ëŠ” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒ 4ê°œë‹¤.

1. **ê·¸ë˜í”„ ë¹Œë”** (`./object_detection/models/code`)  
   â†’ object detection ëª¨ë¸ ì•„í‚¤í…ì²˜ â†’ Neural architecture graph (`nodes feature(X)`, `edges(E)`, `Operator(O)`)

2. **ì›¨ì´íŠ¸ ì¶”ì¶œê¸°** (`weight_extractor.py`)  
   â†’ ê° ë…¸ë“œ(Node)ì— ëŒ€í•´ effective weight $(W_i)$ ì¶”ì¶œ

3. **Flow ì‹œë®¬ë ˆì´í„°** (`Flow_Surrogate_Generator.py`)  
   * Type of Parameter-Gate  
   â†’ $(W_i)$ë“¤ë¡œë¶€í„° ëŒ€í‘œê°’ $(e_i)$ ê³„ì‚°  
   â†’ z-score â†’ gate $(g_i)$  
   â†’ **Gate íƒ€ì… GÂ¹/GÂ²/GÂ³ ì¤‘ ì„ íƒ ê°€ëŠ¥**
   * Gating pattern  
   â†’ DAGì™€ gateë¥¼ ì´ìš©í•´ information flowë¥¼ í˜ë¦¼  
   â†’ **gating íŒ¨í„´(Outgoing / Incoming)ì„ ì„ íƒí•´ì„œ ì ìš©** 

4. **Surrogate ê³„ì‚° ë˜í¼**  
   â†’ `model`ì„ ë°›ì•„ ìœ„ 1â€“3ë‹¨ê³„ë¥¼ í˜¸ì¶œí•´ $(s(M))$ ë°˜í™˜

---

## 1. íŒŒì¼/ë””ë ‰í† ë¦¬ êµ¬ì¡°

```text
PG-Flow/
â”œâ”€â”€ object_detection_models/
â”‚   â”œâ”€â”€ code/                     # ëª¨ë¸ â†’ DAG ì½”ë“œ
â”‚   â”œâ”€â”€ Graph/                    # ìƒì„±ëœ ì•„í‚¤í…ì²˜ë³„ DAG
â”‚   â””â”€â”€ Ops.json                  # Op ë¦¬ìŠ¤íŠ¸
â”œâ”€â”€ parameter_regenerator/        # ì•„í‚¤í…ì²˜ë³„ ì›¨ì´íŠ¸ì— ëŒ€í•´ DAGì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
â””â”€â”€ compute_surrogates.py         # ì—¬ëŸ¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ì›¨ì´íŠ¸ì— ëŒ€í•´ s(M) ê³„ì‚°
```

---

## 2. ì•„í‚¤í…ì²˜ë³„ ê·¸ë˜í”„ ì„¤ê³„ (`./object_detection_models/code/`)

### 2.1 Operator ë¦¬ìŠ¤íŠ¸ ì •ì˜

- `0: Conv_1x1`
- `1: Conv_3x3`
- `2: Conv_3x3_DS`
- `3: Conv_7x7_DS`
- `4: Conv_3x3_Dil`
- `5: DWConv_3x3`
- `6: DWConv_3x3_DS`
- `7: DWConv_5x5`
- `8: MaxPool_2x2`
- `9: MaxPool_SPP`
- `10: AvgPool_Global`
- `11: Upsample_2x`
- `12: RoIAlign_7x7`
- `13: BatchNorm`
- `14: LayerNorm`
- `15: Linear`
- `16: ReLU`
- `17: SiLU`
- `18: Sigmoid`
- `19: Concat`
- `20: Add`
- `21: Split_Half`
- `22: Flatten`
- `23: Reshape_Heads`
- `24: Transpose`
- `25: Gather_TopK`
- `26: MatMul`
- `27: SoftMax`
- `28: Mul`
- `29: Div`
- `30: Sub`
- `31: Exp`

### 2.2 Node ë° Edge ìë£Œêµ¬ì¡° ì •ì˜

**Node êµ¬ì¡°:**
- `id: int` â€“ ê·¸ë˜í”„ ë‚´ node ID  
- `op_idx: int` â€“ nodeë³„ Operator ID  
- `op_name: str` â€“ Operator ì´ë¦„  
- `annotation: str` â€“ ê¸°íƒ€ ì„¤ëª…  

**Edge êµ¬ì¡°:**
- $[id_i, id_j]$ â€“ $node_i$(ì •ë³´ì¶œë ¥ ë…¸ë“œ) â†’ $node_j$(ì •ë³´ì…ë ¥ ë…¸ë“œ)

### 2.3 Architecture ì¢…ë¥˜

**YOLO Family (Real-time One-stage)**
- YOLOv8 (Anchor-free, SOTA): YOLOv8-n, YOLOv8-m
- YOLOv5 (Anchor-based, Industry Standard): YOLOv5-s, YOLOv5-x
- YOLOX (Decoupled Head, Anchor-free): YOLOX-s, YOLOX-l

**R-CNN Family (Two-stage)**
- Faster R-CNN (Standard Baseline): R50-FPN, R101-FPN
- Cascade R-CNN (High Quality): R50-FPN

**Legacy One-stage (Baselines)**
- SSD: SSD300-VGG16, MobileNetV2-SSDLite
- RetinaNet: R50-FPN (Focal Loss Base)

**EfficientDet (Scalable)**
- EfficientDet: D0, D3 (BiFPN + Compound Scaling)

**Transformer (End-to-End)**
- DETR: R50 (Encoder-Decoder Attention)

---

## 3. ê³µí†µ ì›¨ì´íŠ¸ êµí™˜ í¬ë§·(UWEF) ì •ì˜ (`./parameter_regenerator/`)

### 3.1 íŒŒì¼ êµ¬ì¡° (Root Schema)

íŒŒì¼ì˜ ìµœìƒìœ„ ë£¨íŠ¸ëŠ” ë©”íƒ€ë°ì´í„°(meta)ì™€ ê°€ì¤‘ì¹˜ ë°ì´í„°(node_weights) ë‘ ê°€ì§€ í‚¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```json
{
  "meta": {
    "architecture": "string",
    "format_version": "string",
    "source_framework": "string",
    "created_at": "string"
  },
  "node_weights": {
    "NODE_ID_1": { },
    "NODE_ID_2": { },
    "...": "..."
  }
}
```

### 3.2 ë…¸ë“œ ê°€ì¤‘ì¹˜ ê°ì²´ (node_weights)

ê·¸ë˜í”„ ì •ì˜ íŒŒì¼(*_graph.json)ì— ëª…ì‹œëœ Node IDë¥¼ Keyë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ë…¸ë“œëŠ” í•´ë‹¹ ì—°ì‚°ì— í•„ìš”í•œ í…ì„œë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

#### 3.2.1 ê°œë³„ ë…¸ë“œ êµ¬ì¡°

```json
{
  "0": {
    "op_type": "Conv_3x3",
    "has_weight": true,
    "tensors": {
      "weight": { },
      "bias": { },
      "running_mean": { },
      "running_var": { }
    }
  }
}
```

### 3.3 í…ì„œ ê°ì²´ (tensors)

ì‹¤ì œ ê°€ì¤‘ì¹˜ ê°’ì„ ë‹´ëŠ” ê°ì²´ì…ë‹ˆë‹¤. í…ì„œì˜ **ì°¨ì› ì •ë³´(Shape)**ì™€ **ë°ì´í„°(Data)**ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

```json
{
  "weight": {
    "dtype": "float32",
    "shape": [64, 3, 3, 3],
    "data": [0.12, -0.05, 0.0, "..."]
  }
}
```

dataëŠ” ë‹¤ì°¨ì› í…ì„œë¥¼ view(-1) ë˜ëŠ” flatten()í•˜ì—¬ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ë¡œë“œ ì‹œ shapeë¥¼ ì´ìš©í•´ ë³µì›í•©ë‹ˆë‹¤.

### 3.4 ì—°ì‚° íƒ€ì…ë³„ í‘œì¤€ ìŠ¤í‚¤ë§ˆ

ë³€í™˜ ì½”ë“œ ì‘ì„± ì‹œ, ì—°ì‚° íƒ€ì…ì— ë”°ë¼ ë‹¤ìŒ í‚¤(Key) ì´ë¦„ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**A. ì»¨ë³¼ë£¨ì…˜ / ì„ í˜• ë ˆì´ì–´ (Conv2d, Linear)**
- í•„ìˆ˜ í…ì„œ: weight
- ì„ íƒ í…ì„œ: bias
- Shape ê·œì¹™:
  - Conv: [Out_Channels, In_Channels, Kernel_H, Kernel_W]
  - Linear: [Out_Features, In_Features]

**B. ì •ê·œí™” ë ˆì´ì–´ (BatchNorm, LayerNorm)**
- í•„ìˆ˜ í…ì„œ:
  - weight: Scale íŒŒë¼ë¯¸í„° ($\gamma$)
  - bias: Shift íŒŒë¼ë¯¸í„° ($\beta$)
- ì„ íƒ í…ì„œ (BN):
  - running_mean: ì´ë™ í‰ê· 
  - running_var: ì´ë™ ë¶„ì‚°

**C. ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ì—°ì‚° (ReLU, Pooling, Add)**
- has_weight: false
- tensors: {} (ë¹ˆ ê°ì²´)

### 3.5 íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ê·œì¹™ (Implementation Rules)

ë³€í™˜ê¸°(Exporter) êµ¬í˜„ ì‹œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™ì…ë‹ˆë‹¤.

**1. Transformer Q/K/V ë¶„í•  ì €ì¥ ì›ì¹™:**

í”„ë ˆì„ì›Œí¬ ë‚´ë¶€ì—ì„œ í•˜ë‚˜ì˜ í° í…ì„œ(ì˜ˆ: in_proj_weight)ë¡œ í•©ì³ì ¸ ìˆë”ë¼ë„, ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜ì— ë§ì¶° ì˜ë¼ì„œ(Slicing) ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆ: Q_Nodeì—ëŠ” í…ì„œì˜ ì•ë¶€ë¶„(:embed_dim), K_Nodeì—ëŠ” ì¤‘ê°„ ë¶€ë¶„(embed_dim:2*embed_dim)ì„ ì €ì¥í•©ë‹ˆë‹¤.

**2. ê³µìœ  íŒŒë¼ë¯¸í„° ì¤‘ë³µ ì €ì¥ ì›ì¹™ (Deep Copy):**

EfficientDetì˜ Headì²˜ëŸ¼ ì—¬ëŸ¬ ë…¸ë“œê°€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ë”ë¼ë„, ê° ë…¸ë“œ ID í•­ëª©ì— ë°ì´í„°ë¥¼ ì¤‘ë³µí•˜ì—¬ ê¸°ë¡í•©ë‹ˆë‹¤.

ì´ëŠ” ì—”ì§„(Step 2)ì´ ë³µì¡í•œ ì°¸ì¡° ë¡œì§ ì—†ì´ ë…¸ë“œ IDë§Œìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

---

## 4. Gate ê°’ ì„¤ê³„ (Layer 1) â€“ `gate_functions.py`

### 4.0 Gate ì„¤ê³„ ì›ì¹™

- gateëŠ” **ì˜¤ì§ weight $(W)$** ë§Œ ì‚¬ìš©
- Gate(W)ëŠ” ë‹¤ìŒì„ ë§Œì¡±:
  - (1) **ìƒëŒ€ì  í¬ê¸°**ë§Œ ì‚¬ìš© â†’ z-score  
  - (2) ê°’ ë²”ìœ„ **[1âˆ’Î², 1+Î²]** â†’ flowê°€ í„°ì§€ê±°ë‚˜ 0ìœ¼ë¡œ ì£½ì§€ ì•Šê²Œ  
  - (3) weight í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ gate ì¦ê°€ (ë‹¨ì¡° ì¦ê°€)

ì´ë¥¼ ìœ„í•´ ì„¸ ê°€ì§€ Gate íƒ€ì…ì„ ì •ì˜:

```python
s_prime = compute_pgflow_surrogate(
    model,
    gate_type="rel_norm",
    beta=0.2,
    lam=1.0,
    gating_pattern="outgoing",
)
```

- **GÂ¹: `rel_norm`** â€“ ìƒëŒ€ì ì¸ L2 norm ê¸°ë°˜ (ê¸°ë³¸í˜•)  
- **GÂ²: `scale_norm`** â€“ fan-inì„ ê³ ë ¤í•œ scale-invariant norm  
- **GÂ³: `norm_sparsity`** â€“ norm + sparsityë¡œ dead-layer ì–µì œ

---

## 4.1 GÂ¹: Relative Norm Gate (`rel_norm`)

> **ğŸ’¡ í•µì‹¬ ì»¨ì…‰**
> "í‰ê· ë³´ë‹¤ Weight **ì—ë„ˆì§€**ê°€ í° ëª¨ë“ˆì€ Gateë¥¼ ì—´ê³ ($\uparrow$), ì‘ì€ ëª¨ë“ˆì€ ë‹«ëŠ”ë‹¤($\downarrow$)."

| ë‹¨ê³„ | ìˆ˜ì‹ (Formula) | ì„¤ëª… |
| :--- | :--- | :--- |
| **1. ëŒ€í‘œê°’ ($e_i$)** | $e_i = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{\|\theta_i\|}} + \epsilon \right)$ | ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜($\|\theta_i\|$)ë¡œ ì •ê·œí™”ëœ Frobenius Norm |
| **2. Z-Score ($\hat{e}_i$)** | $\hat{e}_i = \frac{e_i - \mu_e}{\sigma_e}$ | ì „ì²´ ë ˆì´ì–´ ë¶„í¬ ë‚´ì—ì„œì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì‚°ì¶œ |
| **3. Gate ($g_i$)** | $g_i = 1 + \beta \tanh(\lambda \hat{e}_i)$ | $\beta=0.2, \lambda=1.0$ (ì˜ˆì‹œ) |

**ğŸ›  êµ¬í˜„ í•¨ìˆ˜ ë§¤í•‘**
* `compute_node_stats_rel_norm(nodes)` â†’ $\{node_{id}: e_i\}$
* `normalize_stats(stats)` â†’ $\{node_{id}: \hat{e}_i\}$
* `gate_tanh(normed_stats, beta, lam)` â†’ $\{node_{id}: g_i\}$

---

## 4.2 GÂ²: Scale-Invariant Norm Gate (`scale_norm`)

> **ğŸ’¡ í•µì‹¬ ì»¨ì…‰**
> "ë‹¨ìˆœ í¬ê¸°ê°€ ì•„ë‹ˆë¼, **ì…ë ¥(Fan-in) ëŒ€ë¹„** ì–¼ë§ˆë‚˜ í¬ê²Œ í•™ìŠµë˜ì—ˆëŠ”ì§€ë¥¼ ë³¸ë‹¤."

| ë‹¨ê³„ | ìˆ˜ì‹ (Formula) | ì„¤ëª… |
| :--- | :--- | :--- |
| **1. Fan-in** | $fan\_in_i = C_{in} \cdot k^2$ | $W_i \in \mathbb{R}^{C_{out} \times C_{in} \times k \times k}$ ì¼ ë•Œ ì…ë ¥ ìˆ˜ìš© ì˜ì—­ í¬ê¸° |
| **2. ëŒ€í‘œê°’** | $e_i = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{fan\_in_i}} + \epsilon \right)$ | $\|\theta_i\|$ ëŒ€ì‹  **Fan-in**ìœ¼ë¡œ ë‚˜ëˆ„ì–´ Scale-Invariant íŠ¹ì„± í™•ë³´ |
| **3. ì´í›„** | $G^1$ê³¼ ë™ì¼ (Z-score â†’ Gate) | |

**ğŸ›  êµ¬í˜„ í•¨ìˆ˜ ë§¤í•‘**
* `compute_node_stats_scale_norm(nodes)` â†’ $\{node_{id}: e_i\}$
* *Note:* Conv ë ˆì´ì–´ê°€ ì•„ë‹Œ ê²½ìš° $G^1$ ë°©ì‹ìœ¼ë¡œ Fallback ì²˜ë¦¬.

---

## 4.3 GÂ³: Norm + Sparsity Gate (`norm_sparsity`)

> **ğŸ’¡ í•µì‹¬ ì»¨ì…‰**
> "Normì´ ì»¤ë„ ëŒ€ë¶€ë¶„ì´ 0(Dead)ì´ë¼ë©´ ë‚®ê²Œ í‰ê°€í•˜ê³ , **Denseí•˜ê²Œ ì‚´ì•„ìˆëŠ”** ë ˆì´ì–´ë¥¼ ê°•ì¡°í•œë‹¤."

| í•­ëª© | ìˆ˜ì‹ (Formula) | ì„¤ëª… |
| :--- | :--- | :--- |
| **Norm Term** | $e_i^{(N)} = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{\|\theta_i\|}} + \epsilon \right)$ | $G^1$ì˜ ëŒ€í‘œê°’ê³¼ ë™ì¼ |
| **Sparsity Term** | $s_i = \frac{1}{\|\theta_i\|} \sum_{\theta \in i} \mathbf{1}(\|\theta\| < \tau)$ | $\tau \approx 10^{-3}$, Dead Parameter ë¹„ìœ¨ |
| **ìµœì¢… ëŒ€í‘œê°’** | $e_i = e_i^{(N)} - \gamma s_i$ | $\gamma > 0$ (ì˜ˆ: 0.5), Sparsityê°€ ë†’ì„ìˆ˜ë¡ ëŒ€í‘œê°’ ì°¨ê° |

**ğŸ›  êµ¬í˜„ í•¨ìˆ˜ ë§¤í•‘**
* `compute_node_stats_norm_sparsity(nodes, tau, gamma)` â†’ $\{node_{id}: e_i\}$

---

## ğŸ“Š ìš”ì•½: Gate íƒ€ì… ë¹„êµ

| íƒ€ì… | ì½”ë“œëª… (`gate_type`) | ê³ ë ¤ ìš”ì†Œ | ì£¼ìš” íŠ¹ì§• |
| :--- | :--- | :--- | :--- |
| **$G^1$** | `rel_norm` | **ì—ë„ˆì§€ (Norm)** | ê°€ì¥ ê¸°ë³¸ì ì´ë©°, íŒŒë¼ë¯¸í„° ì „ì²´ì˜ í‰ê· ì ì¸ í¬ê¸°ë¥¼ ë°˜ì˜ |
| **$G^2$** | `scale_norm` | **ì…ë ¥ ìŠ¤ì¼€ì¼ (Fan-in)** | Conv í•„í„° êµ¬ì¡°ë¥¼ ê³ ë ¤í•¨. ì…ë ¥ ëŒ€ë¹„ ì¦í­ë¥ ì„ ì¤‘ì‹œ |
| **$G^3$** | `norm_sparsity` | **ë°€ë„ (Density)** | Dead neuronì´ ë§ì€ ë ˆì´ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚®ì¶¤ |

---

## 5. Gate ì ìš© íŒ¨í„´ (Layer 2) â€“ `flow_simulator.py`

Gate ê°’ $(g_i)$ (GÂ¹~GÂ³ ì¤‘ í•˜ë‚˜ë¡œ ê³„ì‚°)ê°€ ì¤€ë¹„ë˜ë©´, **Flow ì•ˆì—ì„œ ì–´ë””ì— ê³±í• ì§€**ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ì´ ì„ íƒì€ `gating_pattern`ìœ¼ë¡œ ì»¨íŠ¸ë¡¤í•©ë‹ˆë‹¤.

```python
s_prime = simulate_pgflow(
    nodes,
    edges,
    gates,
    d_hidden=64,
    gating_pattern="outgoing",
)
```

### 5.1 ê³µí†µ ì…ë ¥

| ë³€ìˆ˜ëª… | íƒ€ì… | ì„¤ëª… |
| :--- | :--- | :--- |
| `nodes` | `List[Node]` | ê·¸ë˜í”„ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ |
| `edges` | `List[(src_id, dst_id)]` | Node ID ê¸°ì¤€ì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ |
| `gates` | `Dict[node_id -> g_i]` | ê³„ì‚°ëœ Gate ê°’ ë”•ì…”ë„ˆë¦¬ |
| `d_hidden` | `int` | íˆë“  ë²¡í„° ì°¨ì› |
| `gating_pattern` | `str` | `"outgoing"` ë˜ëŠ” `"incoming"` |

### 5.2 Gating íŒ¨í„´ ë¹„êµ (PÂ¹ vs PÂ²)

| íŒ¨í„´ | ì½”ë“œëª… | ìˆ˜ì‹ | í•´ì„ |
| :--- | :--- | :--- | :--- |
| **PÂ¹: Outgoing** | `"outgoing"` | $m_i = \sum_{j \in \mathcal{N}_{\text{in}}(i)} g_j f_j$ | **ë³´ë‚´ëŠ” ìª½**($j$)ì´ ì–¼ë§ˆë‚˜ í¬ê²Œ ë§í•˜ëŠ”ì§€ë¥¼ Gateë¡œ ë°˜ì˜ |
| **PÂ²: Incoming** | `"incoming"` | $m_i = g_i \cdot \sum_{j} f_j$ | **ë°›ëŠ” ìª½**($i$)ì´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°›ì•„ë“¤ì¼ì§€ë¥¼ Gateë¡œ ë°˜ì˜ |

### 5.3 Flow êµ¬í˜„ ì ˆì°¨

1. **ì¸ë±ìŠ¤ ë§¤í•‘:** Node.id â†’ 0..N-1 ì¸ë±ìŠ¤ ë³€í™˜
2. **ì—£ì§€ ë³€í™˜:** edges â†’ edges_idx
3. **ì´ˆê¸°í™”:** Input ë…¸ë“œì— `torch.randn` ì´ˆê¸° ë©”ì‹œì§€ í• ë‹¹
4. **ìœ„ìƒ ì •ë ¬:** topological_sort(num_nodes, edges_idx) ìˆ˜í–‰
5. **ìˆœíšŒ ë° ê³„ì‚°:**
   - **Outgoing:** $msgs = f_j \cdot g_j$ í›„ Sum
   - **Incoming:** $\sum(f_j)$ í›„ $\cdot g_i$
6. **ìµœì¢… ì‚°ì¶œ:** Input ë…¸ë“œ ë©”ì‹œì§€ í•©ì‚° ë° ì •ê·œí™”

$$s(M) = \frac{s_{\text{prime}}}{\|s_{\text{prime}}\| + \epsilon}$$




# PG-Flow Implementation Plan
_Norm-based Parameter-Gated Flow Surrogate_

This document organizes a **practical action plan** for implementing the **PG-Flow** methodology in actual code.

Objective:

> For a trained model $(M)$ (e.g., YOLO/DETR/Faster R-CNN, etc.)  
> implement a **parameter-gated flow surrogate** $(s(M))$ calculation module  
> using **architecture DAG + weight-based gate**.

PG-Flow Premises:

- Construct a gate $(g_i)$ using weights.
- Gate is **one of GÂ¹/GÂ²/GÂ³ types**, and **gating pattern is either outgoing or incoming**.

---

## 0. Overall Architecture Overview

There are 4 components to implement:

1. **Graph Builder** (`./object_detection/models/code`)  
   â†’ Object detection model architecture â†’ Neural architecture graph (`nodes feature(X)`, `edges(E)`, `Operator(O)`)

2. **Weight Extractor** (`weight_extractor.py`)  
   â†’ Extract effective weight $(W_i)$ for each node

3. **Flow Simulator** (`Flow_Surrogate_Generator.py`)  
   * Type of Parameter-Gate  
   â†’ Compute representative value $(e_i)$ from $(W_i)$  
   â†’ z-score â†’ gate $(g_i)$  
   â†’ **Selectable among GÂ¹/GÂ²/GÂ³ gate types**
   * Gating pattern  
   â†’ Flow information through DAG and gate  
   â†’ **Apply gating pattern (Outgoing / Incoming)** 

4. **Surrogate Computation Wrapper**  
   â†’ Receive `model` and call steps 1â€“3 to return $(s(M))$

---

## 1. File/Directory Structure

```text
PG-Flow/
â”œâ”€â”€ object_detection_models/
â”‚   â”œâ”€â”€ code/                     # Model â†’ DAG code
â”‚   â”œâ”€â”€ Graph/                    # Generated architecture-specific DAGs
â”‚   â””â”€â”€ Ops.json                  # Operator list
â”œâ”€â”€ parameter_regenerator/        # Convert architecture-specific weights to DAG-compatible format
â””â”€â”€ compute_surrogates.py         # Compute s(M) for multiple model architectures and weights
```

---

## 2. Architecture-Specific Graph Design (`./object_detection_models/code/`)

### 2.1 Operator List Definition

- `0: Conv_1x1`
- `1: Conv_3x3`
- `2: Conv_3x3_DS`
- `3: Conv_7x7_DS`
- `4: Conv_3x3_Dil`
- `5: DWConv_3x3`
- `6: DWConv_3x3_DS`
- `7: DWConv_5x5`
- `8: MaxPool_2x2`
- `9: MaxPool_SPP`
- `10: AvgPool_Global`
- `11: Upsample_2x`
- `12: RoIAlign_7x7`
- `13: BatchNorm`
- `14: LayerNorm`
- `15: Linear`
- `16: ReLU`
- `17: SiLU`
- `18: Sigmoid`
- `19: Concat`
- `20: Add`
- `21: Split_Half`
- `22: Flatten`
- `23: Reshape_Heads`
- `24: Transpose`
- `25: Gather_TopK`
- `26: MatMul`
- `27: SoftMax`
- `28: Mul`
- `29: Div`
- `30: Sub`
- `31: Exp`

### 2.2 Node and Edge Data Structure Definition

**Node Structure:**
- `id: int` â€“ Node ID within the graph  
- `op_idx: int` â€“ Operator ID per node  
- `op_name: str` â€“ Operator name  
- `annotation: str` â€“ Additional description  

**Edge Structure:**
- $[id_i, id_j]$ â€“ $node_i$ (information output node) â†’ $node_j$ (information input node)

### 2.3 Architecture Types

**YOLO Family (Real-time One-stage)**
- YOLOv8 (Anchor-free, SOTA): YOLOv8-n, YOLOv8-m
- YOLOv5 (Anchor-based, Industry Standard): YOLOv5-s, YOLOv5-x
- YOLOX (Decoupled Head, Anchor-free): YOLOX-s, YOLOX-l

**R-CNN Family (Two-stage)**
- Faster R-CNN (Standard Baseline): R50-FPN, R101-FPN
- Cascade R-CNN (High Quality): R50-FPN

**Legacy One-stage (Baselines)**
- SSD: SSD300-VGG16, MobileNetV2-SSDLite
- RetinaNet: R50-FPN (Focal Loss Base)

**EfficientDet (Scalable)**
- EfficientDet: D0, D3 (BiFPN + Compound Scaling)

**Transformer (End-to-End)**
- DETR: R50 (Encoder-Decoder Attention)

---

## 3. Universal Weight Exchange Format (UWEF) Definition (`./parameter_regenerator/`)

### 3.1 File Structure (Root Schema)

The top-level root of the file consists of two keys: metadata (meta) and weight data (node_weights).

```json
{
  "meta": {
    "architecture": "string",
    "format_version": "string",
    "source_framework": "string",
    "created_at": "string"
  },
  "node_weights": {
    "NODE_ID_1": { },
    "NODE_ID_2": { },
    "...": "..."
  }
}
```

### 3.2 Node Weight Object (node_weights)

Use Node IDs specified in the graph definition file (*_graph.json) as keys. Each node contains tensors required for the corresponding operation.

#### 3.2.1 Individual Node Structure

```json
{
  "0": {
    "op_type": "Conv_3x3",
    "has_weight": true,
    "tensors": {
      "weight": { },
      "bias": { },
      "running_mean": { },
      "running_var": { }
    }
  }
}
```

### 3.3 Tensor Object (tensors)

An object containing actual weight values. Includes **dimension information (Shape)** and **data**.

```json
{
  "weight": {
    "dtype": "float32",
    "shape": [64, 3, 3, 3],
    "data": [0.12, -0.05, 0.0, "..."]
  }
}
```

Data stores multidimensional tensors as 1D lists using view(-1) or flatten(). Shape is used to restore the tensor when loaded.

### 3.4 Standard Schema by Operation Type

When writing conversion code, respect the following key names according to operation type.

**A. Convolution / Linear Layers (Conv2d, Linear)**
- Required tensors: weight
- Optional tensors: bias
- Shape rules:
  - Conv: [Out_Channels, In_Channels, Kernel_H, Kernel_W]
  - Linear: [Out_Features, In_Features]

**B. Normalization Layers (BatchNorm, LayerNorm)**
- Required tensors:
  - weight: Scale parameter ($\gamma$)
  - bias: Shift parameter ($\beta$)
- Optional tensors (BN):
  - running_mean: Running mean
  - running_var: Running variance

**C. Operations Without Weights (ReLU, Pooling, Add)**
- has_weight: false
- tensors: {} (empty object)

### 3.5 Special Case Handling Rules (Implementation Rules)

Rules that must be observed when implementing the converter (Exporter).

**1. Transformer Q/K/V Split Storage Principle:**

Even if internally merged into a single large tensor (e.g., in_proj_weight), slice and store according to the graph node definition.

Example: Store the front part (:embed_dim) of the tensor in Q_Node, and the middle part (embed_dim:2*embed_dim) in K_Node.

**2. Shared Parameter Duplicate Storage Principle (Deep Copy):**

Even if multiple nodes share the same parameter (e.g., in EfficientDet Head), record the data redundantly for each node ID entry.

This allows the engine (Step 2) to load data using only the node ID without complex reference logic.

---

## 4. Gate Value Design (Layer 1) â€“ `gate_functions.py`

### 4.0 Gate Design Principles

- Gate uses **only weight $(W)$**
- Gate(W) satisfies:
  - (1) **Use only relative magnitude** â†’ z-score  
  - (2) Value range **[1âˆ’Î², 1+Î²]** â†’ Flow does not explode or die to 0  
  - (3) Gate increases monotonically as weight magnitude increases

To achieve this, three gate types are defined:

```python
s_prime = compute_pgflow_surrogate(
    model,
    gate_type="rel_norm",
    beta=0.2,
    lam=1.0,
    gating_pattern="outgoing",
)
```

- **GÂ¹: `rel_norm`** â€“ Relative L2 norm based (basic form)  
- **GÂ²: `scale_norm`** â€“ Scale-invariant norm considering fan-in  
- **GÂ³: `norm_sparsity`** â€“ Norm + sparsity to suppress dead-layer

---

## 4.1 GÂ¹: Relative Norm Gate (`rel_norm`)

> **ğŸ’¡ Core Concept**
> "Modules with weight **energy** larger than average open the gate ($\uparrow$), modules with smaller energy close it ($\downarrow$)."

| Step | Formula | Description |
| :--- | :--- | :--- |
| **1. Representative Value ($e_i$)** | $e_i = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{\|\theta_i\|}} + \epsilon \right)$ | Frobenius Norm normalized by total parameter count ($\|\theta_i\|$) |
| **2. Z-Score ($\hat{e}_i$)** | $\hat{e}_i = \frac{e_i - \mu_e}{\sigma_e}$ | Relative position within the entire layer distribution |
| **3. Gate ($g_i$)** | $g_i = 1 + \beta \tanh(\lambda \hat{e}_i)$ | $\beta=0.2, \lambda=1.0$ (example) |

**ğŸ›  Implementation Function Mapping**
* `compute_node_stats_rel_norm(nodes)` â†’ $\{node_{id}: e_i\}$
* `normalize_stats(stats)` â†’ $\{node_{id}: \hat{e}_i\}$
* `gate_tanh(normed_stats, beta, lam)` â†’ $\{node_{id}: g_i\}$

---

## 4.2 GÂ²: Scale-Invariant Norm Gate (`scale_norm`)

> **ğŸ’¡ Core Concept**
> "Not just absolute magnitude, but **relative to input (Fan-in)** how much was learned."

| Step | Formula | Description |
| :--- | :--- | :--- |
| **1. Fan-in** | $fan\_in_i = C_{in} \cdot k^2$ | Input receptive field size when $W_i \in \mathbb{R}^{C_{out} \times C_{in} \times k \times k}$ |
| **2. Representative Value** | $e_i = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{fan\_in_i}} + \epsilon \right)$ | Divide by **Fan-in** instead of $\|\theta_i\|$ to ensure scale-invariance |
| **3. Afterwards** | Same as $G^1$ (Z-score â†’ Gate) | |

**ğŸ›  Implementation Function Mapping**
* `compute_node_stats_scale_norm(nodes)` â†’ $\{node_{id}: e_i\}$
* *Note:* Fallback to $G^1$ approach for non-Conv layers.

---

## 4.3 GÂ³: Norm + Sparsity Gate (`norm_sparsity`)

> **ğŸ’¡ Core Concept**
> "Even if Norm is large, if most values are 0 (Dead), evaluate low; emphasize **densely alive** layers."

| Item | Formula | Description |
| :--- | :--- | :--- |
| **Norm Term** | $e_i^{(N)} = \log\left( \frac{\lVert W_i \rVert_F}{\sqrt{\|\theta_i\|}} + \epsilon \right)$ | Same as $G^1$ representative value |
| **Sparsity Term** | $s_i = \frac{1}{\|\theta_i\|} \sum_{\theta \in i} \mathbf{1}(\|\theta\| < \tau)$ | $\tau \approx 10^{-3}$, ratio of dead parameters |
| **Final Representative Value** | $e_i = e_i^{(N)} - \gamma s_i$ | $\gamma > 0$ (e.g., 0.5), deduct representative value when sparsity is high |

**ğŸ›  Implementation Function Mapping**
* `compute_node_stats_norm_sparsity(nodes, tau, gamma)` â†’ $\{node_{id}: e_i\}$

---

## ğŸ“Š Summary: Gate Type Comparison

| Type | Code Name (`gate_type`) | Consideration Factors | Key Characteristics |
| :--- | :--- | :--- | :--- |
| **$G^1$** | `rel_norm` | **Energy (Norm)** | Most basic, reflects average magnitude of parameters |
| **$G^2$** | `scale_norm` | **Input Scale (Fan-in)** | Accounts for Conv filter structure; emphasizes amplification relative to input |
| **$G^3$** | `norm_sparsity` | **Density** | Lowers importance of layers with many dead neurons |

---

## 5. Gate Application Pattern (Layer 2) â€“ `flow_simulator.py`

Once gate values $(g_i)$ (computed from one of GÂ¹~GÂ³) are ready, decide **where in the Flow to multiply them**. This choice is controlled by `gating_pattern`.

```python
s_prime = simulate_pgflow(
    nodes,
    edges,
    gates,
    d_hidden=64,
    gating_pattern="outgoing",
)
```

### 5.1 Common Inputs

| Variable Name | Type | Description |
| :--- | :--- | :--- |
| `nodes` | `List[Node]` | List of graph nodes |
| `edges` | `List[(src_id, dst_id)]` | Edge list based on node IDs |
| `gates` | `Dict[node_id -> g_i]` | Dictionary of computed gate values |
| `d_hidden` | `int` | Hidden vector dimension |
| `gating_pattern` | `str` | `"outgoing"` or `"incoming"` |

### 5.2 Gating Pattern Comparison (PÂ¹ vs PÂ²)

| Pattern | Code Name | Formula | Interpretation |
| :--- | :--- | :--- | :--- |
| **PÂ¹: Outgoing** | `"outgoing"` | $m_i = \sum_{j \in \mathcal{N}_{\text{in}}(i)} g_j f_j$ | Gate reflects **how loudly the sender** ($j$) speaks |
| **PÂ²: Incoming** | `"incoming"` | $m_i = g_i \cdot \sum_{j} f_j$ | Gate reflects **how much the receiver** ($i$) accepts |

### 5.3 Flow Implementation Procedure

1. **Index Mapping:** Convert node.id â†’ 0..N-1 indices
2. **Edge Conversion:** edges â†’ edges_idx
3. **Initialization:** Assign initial message `torch.randn` to input nodes
4. **Topological Sort:** Perform topological_sort(num_nodes, edges_idx)
5. **Iteration and Computation:**
   - **Outgoing:** $msgs = f_j \cdot g_j$ then Sum
   - **Incoming:** Sum$(f_j)$ then $\cdot g_i$
6. **Final Output:** Aggregate and normalize input node messages

$$s(M) = \frac{s_{\text{prime}}}{\|s_{\text{prime}}\| + \epsilon}$$